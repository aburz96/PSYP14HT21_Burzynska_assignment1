---
title: "Assignment 1 ZK"
author: "Aleksandra Burzynska"
date: "11/21/2021"

---

##  Assignment part 1
## import data  sample 1
```{r}
data_sample_1 =read.csv("https://tinyurl.com/ha-dataset1")
library(psych)
library(tidyverse) 
library(gridExtra)
library(data.table)
library(car)
library(boot)
library(lmboot) 
library(r2glmm) 
library(lme4) 
library(lmerTest) 
library(MuMIn)
library(lm.beta)
library(influence.ME) 
library(lattice) 
library(lme4) 
library(lmerTest)

coef_table = function(model){ 
  require(lm.beta) 
  mod_sum = summary(model) 
  mod_sum_p_values = as.character(round(mod_sum$coefficients[,4], 3)) 
  mod_sum_p_values[mod_sum_p_values != "0" & mod_sum_p_values != "1"] = substr(mod_sum_p_values[mod_sum_p_values != "0" & mod_sum_p_values != "1"], 2, nchar(mod_sum_p_values[mod_sum_p_values != "0" & mod_sum_p_values != "1"])) 
  mod_sum_p_values[mod_sum_p_values == "0"] = "<.001" 
  
  
  mod_sum_table = cbind(as.data.frame(round(cbind(coef(model), confint(model), c(0, lm.beta(model)$standardized.coefficients[c(2:length(model$coefficients))])), 2)), mod_sum_p_values) 
  names(mod_sum_table) = c("b", "95%CI lb", "95%CI ub", "Std.Beta", "p-value") 
  mod_sum_table["(Intercept)","Std.Beta"] = "0" 
  return(mod_sum_table) 
} 


view(data_sample_1)
#excluding outliers
data_sample_1 = data_sample_1 %>% #dont need to call it anything else,just equal the same name

slice(-c(34, 88)) 

View(data_sample_2) 
head(data_sample_1)
nrow(data_sample_1)
names(data_sample_1)
```
## Model 1
```{r}
Model_1<-lm(pain~age+sex,data=data_sample_1)
summary(Model_1)
summary(Model_1)$adj.r.squared
AIC(Model_1)
confint(Model_1)
coef_table(Model_1)
```
##  Model 2

```{r}
Model_2<-lm(pain~age+sex+STAI_trait+pain_cat+mindfulness+cortisol_serum,data=data_sample_1)
summary(Model_2)
summary(Model_2)$adj.r.squared
AIC(Model_2)
anova(Model_2)
confint(Model_2)
coef_table(Model_2)
```

## check the out outliers

```{r}
library(tidyverse)
library(broom)
model.diag.metrics<-augment(Model_1)
head(model.diag.metrics)
ggplot(model.diag.metrics, aes(age, pain)) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  geom_segment(aes(xend = age, yend = .fitted), color = "red", size = 0.3)
Model_1 %>% 
  plot(which = 5) 

Model_2 %>% 
  plot(which = 5) 

Model_1 %>% 
  plot(which = 4)

Model_2%>% 
  plot(which =4)



```


##  Assumptions of linear regression
##Normality of residuals
```{r}
plot(Model_1, 2)

plot(Model_2, 2)

describe(residuals(Model_1))
describe(residuals(Model_2))
```

The QQ plot of residuals can be used to visually check the normality assumption. The normal probability plot of residuals should approximately follow a straight line.
In our example, all the points fall approximately along this reference line, so we can assume normality.

## Linearity of the data
```{r}
Model_1 %>%
  residualPlots()
Model_2 %>%
  residualPlots()

```

Ideally, the residual plot will show no fitted pattern. That is, the red line should be approximately horizontal at zero. The presence of a pattern may indicate a problem with some aspect of the linear model.

## Homogeneity of variance
```{r}
plot(Model_1,3)
plot(Model_2,3)

Model_2 %>% 
  plot(which = 3) 

Model_2%>% 
  ncvTest() # NCV test 

Model_2 %>% 
  bptest() # Breush-Pagan test 


Model_1 %>% 
  plot(which = 3) 

Model_1 %>% 
  ncvTest() # NCV test 

Model_1%>% 
  bptest() # Breush-Pagan test 


```

This plot shows if residuals are spread equally along the ranges of predictors. It’s good if you see a horizontal line with equally spread points. In our example, this is not the case.

##  Multicollinearity in data sets
```{r}
car::vif(Model_1)
car::vif(Model_2)

```

summary(Model_1)
summary(Model_2)


AIC(Model_1) 
AIC(Model_2) 


anova(Model_1, Model_2)





Here all values of VIF is less than 5 so multicollinearity does not exist in this data set
##  Assignment part 2

```{r}
data_sample_1 =read.csv("https://tinyurl.com/ha-dataset1")
head(data_sample_1)
nrow(data_sample_1)
names(data_sample_1)

```
## Stepwise regression model 1
```{r}
fullmodel_1  <-lm(pain~age+sex+STAI_trait+pain_cat+mindfulness+cortisol_serum+weight+IQ+household_income, data =data_sample_1)
model_1<-step(fullmodel_1, direction = "backward", trace=FALSE )
summary(model_1)
library(car)
library(lattice)
library(carData)
library(caret)
durbinWatsonTest(model_1)
anova(model_1)
confint(model_1)
AIC(model_1)
car::vif(model_1)

```
##  Stepwise regression model2 using new data

```{r}
data_sample_2 =read.csv("https://tinyurl.com/87v6emky")
head(data_sample_2)
names(data_sample_2)
```

## stepwise Regression Model2 using new data
```{r}
fullmodel_2  <-lm(pain~age+sex+STAI_trait+pain_cat+mindfulness+cortisol_serum+weight+IQ+household_income, data =data_sample_2)
model_2<-step(fullmodel_2, direction = "backward", trace=FALSE )
summary(model_2)
anova(model_2)
AIC(model_2)
car::vif(model_2)
```

##  Assignment part 3

```{r}
data_file_3=read.csv("https://tinyurl.com/b385chpu")
data_file_4=read.csv("https://tinyurl.com/4f8thztv")
names(data_file_3)
str(data_file_3)
head(data_file_3)
```
##  Fitt a mixed model
```{r}
 library(Matrix)
 library(lme4)
library(Rcpp)
mixed.lmer<-lmer(pain~sex+age+(1|hospital),data_file_3)
summary(mixed.lmer)
anova(mixed.lmer)
AIC(mixed.lmer)
BIC(mixed.lmer)

```
We can see the variance for hospital = 0.1631. hospital id  is clearly important: they explain a lot of variation. How do we know that? We can take the variance for the hospital id and divide it by the total variance:

## variance

```{r}
variance<-0.1631/(0.1631+1.7778)
variance
```
So the differences between hospital id explain ~8.4% of the variance that’s “left over” after the variance explained by our fixed effects

## R square

```{r}
RSS<-1.7778
TSS<-1.7778+0.1631
R_Square<-1-(RSS/TSS)
R_Square
```


## As always, it’s good practice to have a look at the plots to check our assumptions:
```{r}
plot(mixed.lmer)  # looks alright, no patterns evident

```


##  Q plot
```{r}
qqnorm(resid(mixed.lmer))
qqline(resid(mixed.lmer)) 

```


##  Plotting model predictions

```{r}
library(ggeffects)
mixed.lmer2<-lmer(pain~sex+age++STAI_trait+pain_cat+mindfulness+cortisol_serum+cortisol_saliva+(1|hospital),data_file_3)
summary(mixed.lmer2)
anova(mixed.lmer2)
AIC(mixed.lmer2)
BIC(mixed.lmer2)

```

## R square
```{r}
RSS<-0.1771
TSS<-0.1771+1.2055
R_Square<-1-(RSS/TSS)
R_Square
```

##  prediction
```{r}
predict1<- predict(mixed.lmer)
predict1
predict2<- predict(mixed.lmer2)
predict2
```
